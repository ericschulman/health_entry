{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0501d9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from functools import partial, reduce\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9885911e",
   "metadata": {},
   "source": [
    "## Preprocess numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eefe4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_info = pd.read_csv('../data/processed_data/plan_summary.csv')\n",
    "plan_data = pd.read_csv('../data/data_2016/Plan_Attributes_PUF_2016.csv',encoding='cp1252')\n",
    "\n",
    "act_value = plan_data[['IssuerActuarialValue','AVCalculatorOutputNumber']].copy()\n",
    "act_value =  act_value.fillna(0)\n",
    "act_value['IssuerActuarialValue'] = act_value['IssuerActuarialValue'].apply( lambda x: float(str(x).replace('%','')) )\n",
    "act_value['AVCalculatorOutputNumber'] = act_value['AVCalculatorOutputNumber']*100\n",
    "act_value['act_value'] = act_value.max(axis=1)\n",
    "\n",
    "plan_data['act_value'] = act_value['act_value']\n",
    "\n",
    "good_columns = column_info['Column_Name'][ (column_info['Missing_Values'] <= 3000) & (column_info['Missing_Values'] >= 0) & (column_info['Unique_Values'] >= 90) \n",
    "                  & (column_info['Unique_Values'] <= 370) ]\n",
    "good_columns = list(good_columns) + ['act_value']\n",
    "\n",
    "#fix the set of columns to remove bad ones\n",
    "bad_columns = ['SBCHavingSimplefractureCoinsurance', \n",
    " 'SBCHavingSimplefractureCopayment', 'SBCHavingSimplefractureDeductible', 'ServiceAreaID']\n",
    "good_columns = list(set(good_columns) - set(bad_columns))\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/4703390/how-to-extract-a-floating-number-from-a-string\n",
    "clean_columns = plan_data[['ServiceAreaId','IssuerId']].copy()\n",
    "\n",
    "for column in good_columns:\n",
    "    #try to extract the text\n",
    "    test_column = plan_data[column].copy()\n",
    "    test_column = test_column.fillna(0)\n",
    "    \n",
    "    #if its just an int go with that\n",
    "    int_column = test_column.apply(lambda x : float( (re.findall(r'^-?\\d+(?:\\.\\d+)$',str(x)) +['0']) [0]) )\n",
    "    if int_column.nunique() >= 3:\n",
    "        clean_columns[column] = int_column\n",
    "        \n",
    "    #otherwise try something else\n",
    "    dollar_column = test_column.apply(lambda x : int((re.findall('\\\\$(\\\\d+)',str(x) ) +['0'])[0]) )\n",
    "    if dollar_column.nunique() >= 3:\n",
    "        clean_columns[column] = dollar_column\n",
    "\n",
    "print(clean_columns.columns)\n",
    "\n",
    "#IssuerActuarialValue, AVCalculatorOutputNumber\n",
    "\n",
    "clean_columns.describe()\n",
    "clean_columns.to_csv('../data/processed_data/issuer_numeric.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68647014",
   "metadata": {},
   "source": [
    "## Issuer characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b51414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_df = pd.read_csv('../data/data_2016/Plan_Attributes_PUF_2016.csv',encoding='cp1252')\n",
    "plan_summary = pd.read_csv('../data/processed_data/plan_summary.csv', index_col=None)\n",
    "\n",
    "print(plan_df.shape)\n",
    "\n",
    "# Treat columns as continous\n",
    "cleaned_plan_df = plan_df[no_missing_values]\n",
    "continuous = ['FirstTierUtilization','BeginPrimaryCareCostSharingAfterNumberOfVisits',\n",
    "              'BeginPrimaryCareDeductibleCoinsuranceAfterNumberOfCopays']\n",
    "cleaned_plan_df['FirstTierUtilization'] = cleaned_plan_df['FirstTierUtilization'].str.replace('%','')\n",
    "cleaned_plan_df[continuous] = cleaned_plan_df[continuous].astype(str).astype(float) \n",
    "\n",
    "# Get CSRVariationType binary\n",
    "def largest_dummies(cleaned_plan_df, col_name,number_dummies):\n",
    "    categories = cleaned_plan_df[col_name].value_counts()\n",
    "    largest = categories.head(number_dummies).index.to_list()\n",
    "    rest = categories[number_dummies:].to_list()\n",
    "    cleaned_plan_df[col_name + 'Other'] = 1\n",
    "    for j in range(len(largest)):\n",
    "        category_name = largest[j]\n",
    "        cleaned_plan_df.loc[ cleaned_plan_df[col_name] == largest[j], col_name+str(category_name)] = 1\n",
    "        cleaned_plan_df.loc[ cleaned_plan_df[col_name] != largest[j], col_name+str(category_name)] = 0\n",
    "        cleaned_plan_df.loc[ cleaned_plan_df[col_name] == largest[j], col_name + 'Other'] = 0\n",
    "    return cleaned_plan_df\n",
    "\n",
    "cleaned_plan_df = largest_dummies(cleaned_plan_df, 'CSRVariationType',4)\n",
    "cleaned_plan_df = largest_dummies(cleaned_plan_df, 'IssuerId',10)\n",
    "cleaned_plan_df = largest_dummies(cleaned_plan_df, 'StateCode',50)\n",
    "\n",
    "dummy_cols = ['BusinessYear',\n",
    " 'SourceName',\n",
    " 'MarketCoverage',\n",
    " 'DentalOnlyPlan',\n",
    " 'IsNewPlan',\n",
    " 'PlanType',\n",
    " 'MetalLevel',\n",
    " 'QHPNonQHPTypeId',\n",
    " 'CompositeRatingOffered',\n",
    " 'ChildOnlyOffering',\n",
    " 'OutOfCountryCoverage',\n",
    " 'OutOfServiceAreaCoverage',\n",
    " 'NationalNetwork',\n",
    " 'MultipleInNetworkTiers',\n",
    " 'InpatientCopaymentMaximumDays']\n",
    "cleaned_plan_df = pd.get_dummies(cleaned_plan_df, columns = dummy_cols)\n",
    "cleaned_plan_df\n",
    "\n",
    "issuer_numeric = pd.read_csv('../data/processed_data/issuer_numeric.csv')\n",
    "issuer_numeric = issuer_numeric.iloc[: , 1:]\n",
    "merged_df = cleaned_plan_df.merge(issuer_numeric, left_index=True,right_index=True,suffixes=('', '_y'))\n",
    "\n",
    "#clean up the columns \n",
    "relevant_cols = list(merged_df.columns)\n",
    "for word in ['IssuerId','ServiceAreaId','IssuerId_y','ServiceAreaId_y']:\n",
    "    relevant_cols.remove(word)\n",
    "relevant_cols = ['IssuerId','ServiceAreaId','StandardComponentId'] + relevant_cols \n",
    "merged_df = merged_df[relevant_cols]\n",
    "merged_df\n",
    "\n",
    "#drop the dental plan data?\n",
    "print(merged_df.shape)\n",
    "merged_df  = merged_df[plan_df['DentalOnlyPlan']=='No']\n",
    "print(merged_df.shape)\n",
    "\n",
    "merged_df.to_csv('../data/processed_data/issuer_characteristics_v1.csv', index=False)\n",
    "\n",
    "\n",
    "## Adding the HIX data\n",
    "#add in the hix data\n",
    "cms_data = pd.read_csv('../data/processed_data/issuer_characteristics_v1.csv')\n",
    "#clean up planids in the HIX data...\n",
    "hix_data = pd.read_csv('../data/data_2016/plans_2016.csv')\n",
    "hix_data['PLANID2'] = hix_data['PLANID'] #+ new_plan_df['AREA'].apply(lambda x : '-' + x[-2:])\n",
    "hix_data['PLANID2'] = hix_data['PLANID2'].apply(lambda x : x[:-3])\n",
    "no_dash = hix_data['PLANID'].apply(lambda x : x.find('-') == -1 )\n",
    "hix_data['PLANID2'][no_dash] = hix_data['PLANID'][no_dash]\n",
    "\n",
    "hix_data = hix_data[['PLANID2','PREMIC', 'PREMI27', 'PREMI50', 'PREMI2C30', 'PREMC2C30']]\n",
    "hix_data = hix_data.fillna(0)\n",
    "\n",
    "#take the median for a plan... unclear what area means...\n",
    "hix_data = hix_data.groupby('PLANID2',as_index=False).median()\n",
    "\n",
    "print(hix_data['PLANID2'].nunique())\n",
    "print(hix_data['PREMI50'].nunique())\n",
    "\n",
    "#first merge on ones where we know the service area?\n",
    "merged_df = cms_data.merge(hix_data,how='left', left_on=['StandardComponentId'], \n",
    "                              right_on=['PLANID2'])\n",
    "merged_df = merged_df.fillna(0)\n",
    "\n",
    "print(merged_df['PLANID2'].nunique())\n",
    "print(merged_df['PREMI50'].nunique())\n",
    "\n",
    "merged_df = merged_df.drop(labels=['PLANID2'],axis=1)\n",
    "merged_df.to_csv('../data/processed_data/issuer_characteristics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6407b4d3",
   "metadata": {},
   "source": [
    "## County Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1094a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/data_2016/\"\n",
    "acs1 = pd.read_csv(data_path + 'ACSDP5Y2016.DP03_data_with_overlays_2022-03-17T201553.csv', na_values=['(X)']) #, encoding='cp1252'\n",
    "acs2 = pd.read_csv(data_path + 'ACSDP5Y2016.DP05_data_with_overlays_2021-12-09T232536.csv', na_values=['(X)']) \n",
    "acs3 = pd.read_csv(data_path + 'ACSST5Y2016.S1701_data_with_overlays_2021-12-12T065723.csv', na_values=['(X)'])\n",
    "\n",
    "#DP03_0095E - health care coverage\n",
    "\n",
    "def remove_cols(df):\n",
    "    \n",
    "    #save county\n",
    "    county = df['GEO_ID']\n",
    "    county = county.str[-5:]\n",
    "    county = county.astype(str) \n",
    "    county = county.loc[1:]\n",
    "    \n",
    "    #find the bad columns\n",
    "    cols = list(df.columns)\n",
    "    trim_cols1 = []\n",
    "    for col in cols:\n",
    "        income_col = 'C01_' in col and 'E' in col\n",
    "        if 'PE' in col or 'DP03_0095E' in col or income_col:\n",
    "            trim_cols1.append(col)\n",
    "\n",
    "    df = df[trim_cols1]\n",
    "    df = df.loc[1:]\n",
    "    df.fillna(0, inplace = True)\n",
    "    \n",
    "    \n",
    "    #filter out bad columns\n",
    "    df['County'] = county\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for i in [acs1,acs2,acs3]:\n",
    "    dfs.append(remove_cols(i))\n",
    "\n",
    "for i in dfs:\n",
    "    print(i.shape)\n",
    "\n",
    "\n",
    "[acs1, acs2,acs3] = dfs\n",
    "merge = partial(pd.merge, on=['County'], how='outer')\n",
    "all_acs = reduce(merge, dfs)\n",
    "all_acs\n",
    "\n",
    "\n",
    "# Comment this section of the code for 2017 data - START\n",
    "csr = pd.read_csv(data_path + 'csrzipcounty2016.csv', na_values=['.'])\n",
    "csr.columns = csr.iloc[2]\n",
    "csr = csr.iloc[4:-1]\n",
    "csr.reset_index(drop=True)\n",
    "csr\n",
    "\n",
    "csr_clean = csr.copy()\n",
    "csr_clean = csr_clean\n",
    "for column in list(csr.columns)[3:]:\n",
    "    csr_clean[column] = csr_clean[column].fillna('0')\n",
    "    csr_clean[column] = csr_clean[column].apply(lambda x: float((str(x)).replace('$','').replace(',','')) )\n",
    "# Comment this section of the code for 2017 data - END\n",
    "    \n",
    "merged_acs = all_acs.merge(csr_clean, how='left', left_on='County', right_on='FIPS County Code')\n",
    "merged_acs\n",
    "\n",
    "missing_value_df = pd.DataFrame(merged_acs.isnull().sum())\n",
    "missing_value_df.T #= missing_value_df.T\n",
    "\n",
    "merged_acs.fillna(0, inplace = True)\n",
    "merged_acs.to_csv('../data/processed_data/county_characteristics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151bdf35",
   "metadata": {},
   "source": [
    "## Merge characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b064411",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/data_2016/\"\n",
    "enroll_df = pd.read_csv(data_path + '2016-Issuer-Enrollment-Disenrollment-Report.csv',na_values=['*'])\n",
    "enroll_df = enroll_df[['HIOS ID','Policy County FIPS Code','Ever Enrolled Count']].fillna('0')\n",
    "#clean the column\n",
    "enroll_df['Ever Enrolled Count'] = enroll_df['Ever Enrolled Count'].apply(lambda x: int(x.replace(',','')) )\n",
    "\n",
    "service_df = pd.read_csv(data_path + 'ServiceArea_PUF_2016.csv', encoding='cp1252')\n",
    "service_df = service_df[['County','ServiceAreaId','IssuerId']]\n",
    "service_df = service_df.drop_duplicates()\n",
    "issuer_df = pd.read_csv('../data/processed_data/issuer_characteristics.csv')\n",
    "\n",
    "issuer_service = issuer_df.merge(service_df, how='inner', on=['ServiceAreaId', 'IssuerId'])\n",
    "issuer_service = issuer_service[ ~issuer_service['County'].isna() ]\n",
    "\n",
    "#reorder the columns, drop service area\n",
    "col_order= ['IssuerId','County'] + list(issuer_service.columns)[2:-1]\n",
    "issuer_service = issuer_service[ col_order ]\n",
    "\n",
    "#group by county\n",
    "#pre_cols = list(issuer_service.columns)\n",
    "issuer_service = issuer_service.groupby(['IssuerId','County','StandardComponentId'],as_index=False).median()\n",
    "issuer_service_count = issuer_service.groupby(['IssuerId','County'],as_index=False)['StandardComponentId'].count()\n",
    "issuer_service = issuer_service.groupby(['IssuerId','County'],as_index=False).mean()\n",
    "issuer_service['Plan Counts'] = issuer_service_count['StandardComponentId']\n",
    "\n",
    "print(issuer_service.shape)\n",
    "#print(len(pre_cols),len(post_cols))\n",
    "\n",
    "enroll_issuer = enroll_df.merge(issuer_service, how='left', right_on=['County', 'IssuerId'],\n",
    "                                     left_on=['Policy County FIPS Code','HIOS ID'])\n",
    "\n",
    "enroll_issuer['County'] = enroll_issuer['Policy County FIPS Code']\n",
    "enroll_issuer['IssuerId'] = enroll_issuer['HIOS ID']\n",
    "enroll_issuer = enroll_issuer.fillna(0)\n",
    "print(enroll_issuer.shape)\n",
    "\n",
    "county = pd.read_csv('../data/processed_data/county_characteristics.csv')\n",
    "result = enroll_issuer.merge(county, how='left', left_on='County', right_on='County')\n",
    "result =result[~result['FIPS County Code'].isna()]\n",
    "\n",
    "all_cols  = list(result.columns)\n",
    "keys = ['HIOS ID','Policy County FIPS Code','IssuerId','County','State','FIPS County Code','County Name']\n",
    "for key in keys:\n",
    "    all_cols.remove(key)\n",
    "    \n",
    "#delete bad columns/clean up census data\n",
    "all_cols2 = []\n",
    "\n",
    "for col in all_cols:\n",
    "    #fix cols from census data\n",
    "    result[col] = result[col].apply(lambda x : float(str(x).replace('-','0').replace('N','0')))\n",
    "    \n",
    "    #clean up cols with no variance  \n",
    "    if result[col].std() > 0:\n",
    "        all_cols2.append(col)\n",
    "\n",
    "\n",
    "result[keys + all_cols2].to_csv('../data/processed_data/merged_characteristics.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
